{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SOLUTIONS :"
      ],
      "metadata": {
        "id": "uR7zS7FyJ31r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Can we use Bagging for regression problems?\n",
        "\n",
        "-> Yes, Bagging can be used for regression.\n",
        "\n",
        "You train multiple models on bootstrapped samples and average their outputs to get a stable regression prediction."
      ],
      "metadata": {
        "id": "kZMZwGqTK8RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What is the difference between multiple model training and single model training?\n",
        "\n",
        "-> Single model training means using only one machine learning model to learn from the entire dataset. The model alone is responsible for capturing patterns, making predictions, and handling errors. This approach is simple, fast, and easy to interpret, but it also has limitations. If the chosen model overfits, underfits, or fails to understand complex patterns, the overall accuracy drops because there is no backup. In short, the performance completely depends on the strength of that single model.\n",
        "\n",
        "Multiple model training, also known as ensemble learning, uses several models instead of one. Each model is trained separately—sometimes on different subsets of the data—and their predictions are combined, usually by averaging or voting. This approach reduces errors, increases accuracy, and makes predictions more stable because weaknesses of one model are compensated by others. Although it requires more computation and time, it generally produces more reliable and robust results than a single model."
      ],
      "metadata": {
        "id": "AhxFQALRLOTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.  Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "-> Feature randomness in Random Forest means that each tree chooses splits from only a random subset of features, not all features. This forces every tree to learn different patterns, reduces similarity between trees, and helps prevent overfitting. As a result, the final combined model becomes more accurate and stable."
      ],
      "metadata": {
        "id": "FcDv78NoLz63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "-> The OOB (Out-of-Bag) score is an internal accuracy measure in Random Forests that uses the training samples not included in each tree’s bootstrap dataset to evaluate performance. Since each tree is trained on only about 63% of the data, the remaining 37%—called OOB samples—serve as free test data. The model’s predictions on these OOB samples are combined to estimate the overall accuracy, making the OOB score a built-in, reliable alternative to cross-validation."
      ],
      "metadata": {
        "id": "llXcDv6hMToA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "-> You can measure feature importance in a Random Forest by checking how much each feature contributes to reducing impurity or improving prediction accuracy across all trees. During training, every time a feature is used to split a node, it reduces impurity (like Gini or entropy in classification, or variance in regression). These impurity reductions are added up for each feature across all trees and then normalized to give a feature importance score. Another method is permutation importance, where the values of a feature are randomly shuffled; if model accuracy drops significantly, the feature is considered important."
      ],
      "metadata": {
        "id": "DjoK-SPQMsac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Explain the working principle of a Bagging Classifier?\n",
        "\n",
        "-> A Bagging Classifier works by creating many versions of the same base model using different bootstrap samples of the training data and then combining their predictions to make the final decision. Bootstrap sampling means each model is trained on a randomly selected dataset created by sampling with replacement, so every model sees a slightly different view of the data. Because each model learns different patterns, their errors are less correlated. During prediction, the Bagging Classifier takes a majority vote from all individual models to decide the final class label. This process reduces variance, improves stability, and helps prevent overfitting, especially for high-variance models like decision trees."
      ],
      "metadata": {
        "id": "kU3MRf8yM5on"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "-> You can evaluate a Bagging Classifier’s performance using the same methods used for any classification model. First, you split the data into training and testing sets, train the Bagging Classifier on the training set, and then test it on unseen data. Common evaluation metrics include accuracy, precision, recall, F1-score, and confusion matrix to understand how well the model is classifying each class. You can also use cross-validation to get a more reliable performance estimate. If using Random Forest or Bagging with bootstrap samples, the OOB (Out-of-Bag) score can also act as a built-in validation metric to estimate model accuracy without needing a separate test set."
      ],
      "metadata": {
        "id": "w7nCz7XXNH53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. How does a Bagging Regressor work?\n",
        "\n",
        "-> A Bagging Regressor works by training multiple regression models on different bootstrap samples of the training data and then averaging their predictions to get the final output. Each bootstrap sample is created by randomly selecting data points with replacement, so every model sees a slightly different version of the dataset. Because each model learns different patterns and errors, the predictions become more stable when combined. During prediction, all individual regressors make their predictions, and the Bagging Regressor outputs the average of these values. This averaging reduces variance, makes the model less sensitive to noise, and helps prevent overfitting—especially when using high-variance models like decision trees."
      ],
      "metadata": {
        "id": "27uzPEf7NX30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "-> The main advantage of ensemble techniques is that they combine the predictions of multiple models to produce results that are more accurate, more stable, and less prone to overfitting than any single model. By aggregating different models—each with its own strengths and weaknesses—ensemble methods reduce errors, lower variance, and improve generalization, leading to consistently better performance on unseen data."
      ],
      "metadata": {
        "id": "zYOJmRpVNrIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. What is the main challenge of ensemble methods?\n",
        "\n",
        "-> The main challenge of ensemble methods is that they are computationally expensive and more complex compared to single models. Since ensembles train multiple models instead of one, they require more time, memory, and processing power. They can also be harder to interpret, making it difficult to understand how the final prediction is made. This increased complexity and cost are the biggest drawbacks of using ensemble techniques."
      ],
      "metadata": {
        "id": "Hf-eImwDN8MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Explain the key idea behind ensemble techniques?\n",
        "\n",
        "-> The key idea behind ensemble techniques is to combine the predictions of multiple models to produce a final result that is more accurate, stable, and reliable than any single model alone. Each individual model may make errors or capture only part of the pattern, but when many diverse models are brought together—through methods like voting, averaging, bagging, or boosting—their strengths reinforce each other while their weaknesses get reduced. This collective decision-making helps improve generalization and reduces overfitting, leading to better overall performance on unseen data."
      ],
      "metadata": {
        "id": "D1y9VnbGOPYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. What is a Random Forest Classifier.\n",
        "\n",
        "-> A Random Forest Classifier is an ensemble model that builds many decision trees using random subsets of data and features, and predicts the final class using majority voting. It improves accuracy and reduces overfitting compared to a single tree."
      ],
      "metadata": {
        "id": "22MM6aeOOd8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. What are the main types of ensemble techniques.\n",
        "\n",
        "-> The main types of ensemble techniques are:\n",
        "\n",
        "- Bagging (Bootstrap Aggregating)\n",
        "\n",
        "- Boosting (AdaBoost, Gradient Boosting, XGBoost)\n",
        "\n",
        "- Stacking (meta-model combining multiple models)"
      ],
      "metadata": {
        "id": "2YUX6QjfO6yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. What is ensemble learning in machine learning?\n",
        "\n",
        "-> Ensemble learning is the technique of combining multiple models to make a final prediction that is more accurate, stable, and robust than any single model."
      ],
      "metadata": {
        "id": "8OQ06puTPGi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. When should we avoid using ensemble methods.\n",
        "\n",
        "-> Avoid ensemble methods when interpretability, low computation cost, or real-time prediction speed is required, because ensembles are more complex, slower, and harder to understand."
      ],
      "metadata": {
        "id": "La8XZ6D-PRbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. How does Bagging help in reducing overfitting.\n",
        "\n",
        "-> Bagging reduces overfitting by training multiple models on different bootstrap samples and averaging their predictions, which lowers variance and makes the final model more stable."
      ],
      "metadata": {
        "id": "N1h2BwurPgQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "-> Random Forest is better because it reduces overfitting, improves accuracy, and increases stability by combining many trees that learn different patterns through randomness."
      ],
      "metadata": {
        "id": "0fTv-izlPsQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "-> Bootstrap sampling creates many different training subsets by sampling with replacement. Each model trains on a unique dataset, making the ensemble more diverse and reducing variance."
      ],
      "metadata": {
        "id": "8X9-5THMQArt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "-> Ensemble methods are used in:\n",
        "\n",
        "- Fraud detection\n",
        "\n",
        "- Customer churn prediction\n",
        "\n",
        "- Medical diagnosis\n",
        "\n",
        "- Spam detection\n",
        "\n",
        "- Stock market prediction\n",
        "\n",
        "- Image recognition\n",
        "\n",
        "- Recommendation systems"
      ],
      "metadata": {
        "id": "cYpieuiLQNf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "->\n",
        "- Bagging trains many independent models on random subsets and combines them to reduce variance.\n",
        "\n",
        "- Boosting trains models sequentially, where each model focuses on correcting the errors of the previous one, reducing both bias and variance."
      ],
      "metadata": {
        "id": "g0Rl120xQgyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=20, bootstrap=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvswo0kGQ2Fr",
        "outputId": "deb8e4a6-dcbc-4b63-f5f4-a499638339f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=20, bootstrap=True, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSGEID7SSSvp",
        "outputId": "c8dbf0a3-f711-41c2-987b-ae72220cf04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.2692915206048561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "for i, score in enumerate(importances):\n",
        "    print(f\"Feature {i}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0-yvOVtTQkV",
        "outputId": "ce67da34-ee57-4a76-9425-6aa47bd3224f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: 0.0323\n",
            "Feature 1: 0.0111\n",
            "Feature 2: 0.0601\n",
            "Feature 3: 0.0538\n",
            "Feature 4: 0.0062\n",
            "Feature 5: 0.0092\n",
            "Feature 6: 0.0806\n",
            "Feature 7: 0.1419\n",
            "Feature 8: 0.0033\n",
            "Feature 9: 0.0031\n",
            "Feature 10: 0.0164\n",
            "Feature 11: 0.0032\n",
            "Feature 12: 0.0118\n",
            "Feature 13: 0.0295\n",
            "Feature 14: 0.0059\n",
            "Feature 15: 0.0046\n",
            "Feature 16: 0.0058\n",
            "Feature 17: 0.0034\n",
            "Feature 18: 0.0040\n",
            "Feature 19: 0.0071\n",
            "Feature 20: 0.0780\n",
            "Feature 21: 0.0188\n",
            "Feature 22: 0.0743\n",
            "Feature 23: 0.1182\n",
            "Feature 24: 0.0118\n",
            "Feature 25: 0.0175\n",
            "Feature 26: 0.0411\n",
            "Feature 27: 0.1271\n",
            "Feature 28: 0.0129\n",
            "Feature 29: 0.0069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree Regressor\n",
        "\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "print(\"Decision Tree MSE:\", mse_dt)\n",
        "\n",
        "# Random Forest Regressor\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(\"Random Forest MSE:\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjq5FP_TT56x",
        "outputId": "37d77690-3a5d-46a4-c203-734696d1ab10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.5280096503174904\n",
            "Random Forest MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"OOB Score:\", rf_model.oob_score_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC1leb3NUQmA",
        "outputId": "0a41c4e6-082b-4295-ef26-5d54c91468bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9547738693467337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_base = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "bagging_model = BaggingClassifier(estimator=svm_base, n_estimators=10, bootstrap=True, random_state=42)\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRxKdDvbUnUF",
        "outputId": "d43615ac-1386-403f-eacb-8a8cdedd1edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 27.  Train a Random Forest Classifier with different numbers of trees and compare accuracy2\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "n_trees_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in n_trees_list:\n",
        "  rf_model = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "  rf_model.fit(X_train, y_train)\n",
        "  y_pred = rf_model.predict(X_test)\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Number of Trees: {n_trees}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRS-1va1U0JO",
        "outputId": "25ac27f6-ca67-4ac2-b4ad-88ed9ade3a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trees: 10, Accuracy: 0.9649\n",
            "Number of Trees: 50, Accuracy: 0.9708\n",
            "Number of Trees: 100, Accuracy: 0.9708\n",
            "Number of Trees: 200, Accuracy: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "logreg_base = LogisticRegression(max_iter=1000, random_state=42)\n",
        "bagging_model = BaggingClassifier(estimator=logreg_base, n_estimators=10, bootstrap=True, random_state=42)\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_prob = bagging_model.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(\"AUC Score:\", auc_score)\n"
      ],
      "metadata": {
        "id": "0ovQZrxtVHEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf08033e-5132-44d3-8836-58ed608df0b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9977954144620812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 29.  Train a Random Forest Regressor and analyze feature importance scores\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "for i, score in enumerate(importances):\n",
        "  print(f\"Feature {i}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGzoIqBOVZBc",
        "outputId": "f43ab2ec-e6dc-4064-9887-72176a274b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: 0.5260\n",
            "Feature 1: 0.0547\n",
            "Feature 2: 0.0472\n",
            "Feature 3: 0.0300\n",
            "Feature 4: 0.0317\n",
            "Feature 5: 0.1382\n",
            "Feature 6: 0.0861\n",
            "Feature 7: 0.0861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 30.Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier\n",
        "\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_model.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "print(\"Bagging Classifier Accuracy:\", acc_bag)\n",
        "\n",
        "# Random Forest Classifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", acc_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_nqGEWDWC90",
        "outputId": "05d7d9e8-05f1-4188-e1da-477ea914f5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9590643274853801\n",
            "Random Forest Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "'n_estimators': [50, 100, 200],\n",
        "'max_depth': [None, 5, 10],\n",
        "'min_samples_split': [2, 5, 10],\n",
        "'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOhSD32PW6a7",
        "outputId": "ed14a373-2bc9-4334-c4a9-c6430b88a225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Test Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "n_estimators_list = [5, 10, 20, 50]\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42, bootstrap=True)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Number of Estimators: {n}, MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T87eflIW6YH",
        "outputId": "d9fd2628-02c2-4f10-8a7f-9a7262e1e5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Estimators: 5, MSE: 0.3168\n",
            "Number of Estimators: 10, MSE: 0.2862\n",
            "Number of Estimators: 20, MSE: 0.2693\n",
            "Number of Estimators: 50, MSE: 0.2579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 33. Train a Random Forest Classifier and analyze misclassified samples\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Identify misclassified samples\n",
        "\n",
        "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]\n",
        "print(\"Number of Misclassified Samples:\", len(misclassified_indices))\n",
        "print(\"Indices of Misclassified Samples:\", misclassified_indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aikR571W6Vt",
        "outputId": "aafd7646-a28f-4c94-9def-c6ab3e2360f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9707602339181286\n",
            "Number of Misclassified Samples: 5\n",
            "Indices of Misclassified Samples: [8, 20, 77, 82, 164]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(\"Decision Tree Accuracy:\", acc_dt)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_model.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "print(\"Bagging Classifier Accuracy:\", acc_bag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwcatTsMW6S-",
        "outputId": "28a89609-e88f-42c6-a526-df1ed32ae243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9415204678362573\n",
            "Bagging Classifier Accuracy: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. Train a Random Forest Classifier and visualize the confusion matrix\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)\n",
        "disp.plot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "M4-0IMFuW6P9",
        "outputId": "0d88d99f-8435-4432-bd8b-ece6c141c219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7c73d0ff97c0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#36.Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "estimators = [\n",
        "('dt', DecisionTreeClassifier(random_state=42)),\n",
        "('svm', SVC(kernel='rbf', probability=True, random_state=42)),\n",
        "('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "stack_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n",
        "stack_model.fit(X_train, y_train)\n",
        "y_pred_stack = stack_model.predict(X_test)\n",
        "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
        "print(\"Stacking Classifier Accuracy:\", acc_stack)\n",
        "\n",
        "for name, model in estimators:\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  print(f\"{name} Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "-l0cgIyUW6Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #37.Train a Random Forest Classifier and print the top 5 most important features.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1] # Sort descending\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "for i in range(5):\n",
        "  print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKWG1CPW6K1",
        "outputId": "e954d47e-36fe-457e-d85e-7f9b2d94a378"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "mean concave points: 0.1419\n",
            "worst concave points: 0.1271\n",
            "worst area: 0.1182\n",
            "mean concavity: 0.0806\n",
            "worst radius: 0.0780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzwH-52-W6IS",
        "outputId": "4ac659aa-4db2-4db4-e06a-16d1133f34f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.963302752293578\n",
            "Recall: 0.9722222222222222\n",
            "F1-score: 0.967741935483871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "max_depth_values = [None, 2, 4, 6, 8, 10]\n",
        "\n",
        "for depth in max_depth_values:\n",
        "  rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred = rf_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"max_depth={depth}, Accuracy={acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4enGp0CwW6Fe",
        "outputId": "7eb09215-fcc4-4d55-f2c8-6c74eaa4a6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth=10, Accuracy=0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#40.Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "estimators = {\n",
        "\"DecisionTree\": DecisionTreeRegressor(),\n",
        "\"KNeighbors\": KNeighborsRegressor()\n",
        "}\n",
        "\n",
        "for name, base_estimator in estimators.items():\n",
        "  model = BaggingRegressor(estimator=base_estimator, n_estimators=20, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Base Estimator: {name}, MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNuLLhdgVkHG",
        "outputId": "682a8229-0cd8-4a7c-8b75-2a3ad2857633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Estimator: KNeighbors, MSE: 1.1105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", auc_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6CvaMB0c13p",
        "outputId": "cf568ad0-4332-41bb-e8ce-0a86354ca0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9968400940623163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#42.Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "cv_scores = cross_val_score(bagging_model, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_9HNFsodDfd",
        "outputId": "8d80dd62-c174-45da-cbbd-9c85e173c325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.9122807  0.92105263 0.98245614 0.95614035 1.        ]\n",
            "Mean CV Accuracy: 0.9543859649122808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#43.Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "aVDggg8udUF7",
        "outputId": "176c33cc-912b-4ca6-b442-025c7ff5a5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7c73d1b67770>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "estimators = [\n",
        "('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "stack_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n",
        "stack_model.fit(X_train, y_train)\n",
        "y_pred_stack = stack_model.predict(X_test)\n",
        "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
        "print(\"Stacking Classifier Accuracy:\", acc_stack)\n",
        "\n",
        "for name, model in estimators:\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  print(f\"{name} Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "jgkuz8gHdtjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bootstrap_options = [True, False]\n",
        "\n",
        "for bootstrap in bootstrap_options:\n",
        "  model = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
        "n_estimators=20,\n",
        "bootstrap=bootstrap,\n",
        "random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bootstrap={bootstrap}, MSE={mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siCZFeI1eCqC",
        "outputId": "df6a59cf-398d-4157-dd15-2e4f388f443e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap=False, MSE=0.4971\n"
          ]
        }
      ]
    }
  ]
}